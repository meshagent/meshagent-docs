---
title: "Worker"
---

import CliWorkerYaml from "/snippets/examples/cli/worker/meshagent.mdx"
import QueueWorker from "/snippets/examples/python/deployable/queueworker/queueworker.mdx"
import PushQueue from "/snippets/examples/python/deployable/queueworker/push_queue.mdx"
import QueueWorkerYaml from "/snippets/examples/python/deployable/queueworker/meshagent.mdx"

## Overview
A ``Worker`` is a specialized **queue-based agent** that processes messages sent to a MeshAgent room queue. **Other agents or
applications can push tasks to the queue and the Worker will handle them in the background**. This is helpful for
long‑running or asynchronous jobs that shouldn't block an interactive chat agent.

### Two ways to build a Worker
1. **CLI:** Run production-ready workers with a single command. Configure queue, tools, and rules using flags. Ideal for most use cases.
2. **SDK:** Extend the base ``Worker`` class with custom code when you need deeper integrations or specialized behaviors.

### In this guide you will learn
- When to use ``Worker``
- How to run and deploy a ``Worker`` with the MeshAgent CLI
- How to build and deploy a ``Worker`` with the MeshAgent SDK
- How the ``Worker`` works including constructor parameters, lifecycle, processing flow, and hooks

## When to use Worker
- Process background jobs pushed by apps or other agents
- Run long or repetitive jobs off the main chat thread
- Execute non-interactive tasks where no follow up questions are desired
- Batch operations that process multiple items sequentially
- Schedule or trigger work externally and have the agent pick it up

If you need an interactive assistant, use [ChatBot](./chatbot). For real-time speech, see [VoiceBot](./voicebot). For a callable function/agent pattern with structured schemas, see [TaskRunner](./taskrunner).

## Run and deploy a Worker with the CLI
### Step 1: Run a ``Worker`` from the CLI
Let's run a Worker that listens on a queue and can write files to room storage.

```bash bash
# Authenticate to MeshAgent if not already signed in
meshagent setup

# Call a worker into your room
meshagent worker join --room quickstart --agent-name worker --queue tasks --require-storage \
  --room-rules "agents/worker/rules.txt" --rule "Process queued tasks and save results to storage."
```

When you add the `--room-rules "agents/worker/rules.txt"` flag and supply a file path for the rules, the file will be created if it does not already exist, this file is relative to the room storage. The `--queue` flag is required.

> **Tip:** Use `meshagent worker join --help` to see all available tools and options.

### Step 2: Send work to the queue
```bash bash
meshagent room queue send --room=quickstart --queue=tasks \
  --json '{"prompt": "save a poem about ai to poem.txt"}'
```

### Step 3: Package and deploy the worker
Once your worker runs locally, package it as a service so it is always available in the room. 

**Both options below deploy the exact same worker - choose based on your workflow:**
- **Option 1 (`meshagent worker deploy`):** One command that deploys immediately (fastest/easiest approach)
- **Option 2 (`meshagent worker spec` + `meshagent service create`):** Generates a yaml file you can review or futher customize before deploying

**Option 1: Deploy directly**
Use the CLI to automatically deploy the worker to your room. 

```bash bash 
meshagent worker deploy --room quickstart --service-name worker --agent-name worker --queue tasks --require-storage \
  --room-rules "agents/worker/rules.txt" --rule "Process queued tasks and save results to storage."
```

**Option 2: Generate a YAML spec**  Create a `meshagent.yaml` file that defines how our service should run, then deploy the agent to our room. The service spec can be dynamically generated from the CLI by running: 

```bash 
meshagent worker spec --service-name worker --agent-name worker --queue tasks --require-storage \
  --room-rules "agents/worker/rules.txt" --rule "Process queued tasks and save results to storage."
```

Next, copy the output to a `meshagent.yaml` file

<CodeGroup>
    <CliWorkerYaml />
</CodeGroup>

Then deploy it to your Room:
```bash bash
meshagent service create --file meshagent.yaml --room quickstart
```

The `--room` flag is optional. Without it, the worker is deployed at the project level and appears in all rooms in your project.

## Build and deploy a Worker with the SDK
The SDK approach produces the same deployed worker as the CLI examples above. Using the SDK allows you further control to write custom Python code for specialized processing logic, integrations, or behaviors. For most use cases the CLI is sufficient. 

### Step 1: Create a Worker Agent

The sample below shows a Worker that listens on a queue and writes files using the `StorageToolkit`. After starting the
service you can push a message to the queue to trigger the worker.

First create a python file, ``main.py``, and define our ``StorageWorker``:

<CodeGroup>
    <QueueWorker />
</CodeGroup>

### Step 2: Running the Worker

From your terminal inside an activated virtual environment start the service:
```bash bash
meshagent setup # authenticate to meshagent if not already
meshagent service run "main.py" --room=quickstart
```

### Step 3: Sending Work to the Queue

Now that our agent is running, let's send it some work!

**Option 1: Using the MeshAgent CLI**
Use the MeshAgent CLI to directly connect to the room, mint a token, and send a message to the queue.

```bash bash
meshagent room queue send --room=quickstart --queue=storage-worker-queue \
  --json '{"prompt": "save a poem about ai to poem.txt"}'
```

**Option 2: Invoking the queue from code**
Create a python file ``push_queue.py`` and define our function to push a message to the queue.

<CodeGroup>
    <PushQueue />
</CodeGroup>

Make sure your service is still running (``meshagent service run "main.py" --room=quickstart``). From a different tab in your terminal you can run the python file we created to send a task to the queue.

```bash bash
export MESHAGENT_API_KEY="<your_api_key>"
python push_queue.py
```

You'll see logs for the queue activity in the terminal window where your service is running. To verify the results we'll head to MeshAgent Studio where we can see the .txt files.

#### Checking results in the Studio
From [studio.meshagent.com](https://studio.meshagent.com) open the room ``quickstart`` and you'll see our poem about AI in the poems.txt file!

### Step 4: Package and deploy the agent
To deploy your SDK Worker permanently, you’ll need:
1. `A meshagent.yaml` file defining the agent configuration
2. A Docker container with your code

For this example, we’ll use a MeshAgent base image that already contains the code above.

> **Note**: For your own custom agents, create a Dockerfile, build with `docker buildx`, and push to your registry with `--platform linux/amd64`. For example, `docker buildx build -t "<REGISTRY>/<NAMESPACE>/<IMAGE_NAME>:<VERSION_TAG>" --platform linux/amd64 --push`.

**Package the agent**: Create a `meshagent.yaml` file just like we did with the CLI based workers. 

<CodeGroup>
    <QueueWorkerYaml />
</CodeGroup>

**Deploy the agent**: Next from the CLI in the folder where your `meshagent.yaml` file is run:

```bash bash 
meshagent service create --file "meshagent.yaml" --room=quickstart
```

The `Worker` is now deployed to the `quickstart` room! Now the agent will always be available inside the room for us to send queue-based tasks. We can use the same CLI commands or SDK code from above to send messages to the queue. 

## How Worker Works
### Constructor Arguments
A ``Worker`` accepts everything from ``SingleRoomAgent`` (``title``, ``description``, ``requires``, ``labels``). The ``name`` constructor argument is deprecated; agent identity comes from its participant token.

| Argument       | Type                          | Description                                                                                            |
| -------------- | ----------------------------- | ------------------------------------------------------------------------------------------------------ |
| `queue`        | `str`                         | **Required.** Name of the room queue to listen for messages on.                                                       |
| `llm_adapter`  | `LLMAdapter`                  | The adapter to use so the agent works with an LLM. We recommend using the ``OpenAIResponsesAdapter`` from ``meshagent-openai``. |
| `tool_adapter` | `ToolResponseAdapter \| None` | Optional adapter to translate tool outputs into LLM/context messages. We recommend using the ``OpenAIResponsesToolResponseAdapter`` from ``meshagent-openai``.                                 |
| `toolkits`     | `list[Toolkit] \| None`       | Local toolkits always available to this worker (in addition to any `requires`).                        |
| `rules`        | `list[str] \| None`           | Optional system prompt/rules that guide the agent's behavior.                                      |
| `requires`     | `list[Requirement] \| None`   | Schemas/toolkits to install in the room before processing. You can use RequiredSchema, RequiredToolkit to use existing toolkits and schemas that have been registered with the room.                                            |

### Lifecycle Overview

- ``await start(room)``: Connects to the room, installs requirements, and starts the background run() loop that consumes messages from the configured queue.
- ``await stop()``: Signals the loop to stop, awaits the main task, then disconnects cleanly.
- ``room property``: Access the active ``RoomClient`` for queues, storage, messaging, and tools.

### Processing Flow

1. Wait for work (long-polling). The worker listens for jobs using: ``message = await room.queues.receive(name=self._queue, create=True, wait=True)``
    With ``wait=True``, the call long-polls the queue: instead of returning immediately when the queue is empty, it waits asynchronously for a short window until a message arrives (or the wait times out). This is more efficient than tight polling and ensures the worker can pick up jobs immediately when they’re published.

2. Build context and tools. Create a fresh chat context (``init_chat_context()``), apply any rules, and resolve toolkits from requires plus local toolkits.
3. Represent the job in context. By default, ``append_message_context(...)`` serializes the message as a user message (JSON). Override this to customize how the payload is injected.
4. Process the job. ``process_message(...)`` runs the task (default implementation calls your ``llm_adapter.next()`` with the prepared context and toolkits).
5. Handle errors & keep running. Errors are logged. On receive failures, the loop backs off exponentially and retries; otherwise it immediately waits for the next message.


### Key Behaviors and Hooks
- **Queue consumption:** ``run()`` long-polls the queue ``(receive(..., wait=True))``, creating it if needed.
- **Context building:** ``append_message_context(...)`` inserts job data into the chat context (default: dump JSON as a user message).
- **Job execution:** ``process_message(...)`` is the main hook: it prepares context, tools, and (by default) calls the LLM adapter’s ``.next()`` function to work through the task.
- **Resilience:** Errors are logged; ``run()`` applies an exponential backoff between receive retries.

### Key Methods

| Method                                                             | Description                                                        |
| ------------------------------------------------------------------ | ------------------------------------------------------------------ |
| `async def run(room)`                                              | Main loop: receives queue messages and processes them.             |
| `async def process_message(chat_context, room, message, toolkits)` | Core job handler—override for custom behavior.                     |
| `async def append_message_context(room, message, chat_context)`    | How a job is represented in the chat context (override to change). |
| `async def start(room)` / `async def stop()`                       | Start/stop the worker, manage background task.                     |


## Next Steps
Continue Learning about Agents and Explore a variety of other base agents including:
- [ChatBot](./chatbot) for conversational text-based agents
- [VoiceBot](./voicebot) for conversational speech/voice based agents
- [TaskRunner](./taskrunner) for agents that run in the background with defined inputs and outputs
- [MailBot](./mailbot) for email based agents
