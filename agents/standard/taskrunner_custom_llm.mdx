---
title: "Create a Custom LLM TaskRunner"
---

import LLMTaskRunners from "/snippets/examples/python/agents/taskrunner/llm_taskrunners.mdx"
import LLMTaskRunnerService from "/snippets/examples/python/agents/taskrunner/llm_taskrunners_service.mdx"
import InvokeLLMTaskRunner from "/snippets/examples/python/agents/taskrunner/invoking_llm_taskrunner.mdx"

``TaskRunner`` is a base class that lets you bring your own logic — whether that means calling another framework, running business rules, or integrating an LLM. Most often, you’ll extend ``TaskRunner`` with an LLM to build agents that can reason, use tools, and handle complex tasks.

**LLM TaskRunners** are designed for focused execution — they start with a clean context each time, complete the task, and return a result. This makes them ideal when you have a defined workflow or process and simply want the agent to do the work. For example, generating content, processing documents, running scheduled jobs, or handling batch operations.

This differs from chat-based agents (like ``ChatBot``), which maintain conversation history and support iterative, collaborative exchanges. Both can perform complex work and use the same toolkits, but LLM TaskRunners excel when:

- You have well-defined inputs and outputs
- The task doesn’t require multi-turn clarification or refinement with the user
- You need a fresh context window with each execution
- You’re running background jobs, batch tasks, or callable tools for other agents

## Building an LLM TaskRunner
We'll walk through extending the base ``TaskRunner`` with LLM capabilities. This example shows two implementations: 

- **``LLMTaskRunner``**: Defines a fixed output schema at initialization. Best when you always want the same response format. By default, accepts a text prompt and returns a string response.
- **``DynamicLLMTaskRunner``**: Accepts an ``output_schema`` parameter at runtime, letting you change the response structure for each request.

### What You're Adding to the Base TaskRunner
The base ``TaskRunner`` is intentionally simple - it handles the plumbing (schemas, validation, room integration) while you provide the execution logic. This example shows you how to extend it with LLM capabilities, creating task-oriented agents that can reason through problems and return structured results.

We'll expand on the base ``TaskRunner`` by adding: 
1. **Chat Context**: Initialize conversation state using the ``init_chat_context`` method. 
2. **LLM Execution Loop**: Implement an ``ask()`` function that calls ``.next()`` on the ``LLMAdapter``, allowing the model to reason and use tools until the task completes.
3. **Schema Validation**: Validate the LLM output against the declared schema to ensure consistency.


### Constructor Parameters
- **llm_adapter:** a LLM adapter to use to integrate with a LLM. We recommend using the OpenAIResponsesAdapter from ```meshagent-openai```.
- **supports_tools** Whether the agent should support passing a custom set of tools at runtime (optional)
- **tool_adapter:** a custom tool adapter to use to transform tool responses into context messages (optional).
- **toolkits:** used to specify local toolkits for the agent. While it's generally recommended to register toolkits with the room so any agent or user can use them, sometimes you need each agent to have it's own instance of a toolkit, for instance with synchorized document authoring.
- **requires:** a list of requirements for the agent. You can use RequiredSchema, RequiredToolkit to use toolkits and schemas that have been registered with the room with this agent.
- **input_prompt:** Whether the TaskRunner should accept a prompt as input, if true, the input should be in the format ```{ "prompt" : "text" }```.
- **input_schema:** JSON schema describing what arguments your agent accepts. If not provided and input_prompt is True, defaults to a prompt schema accepting text.
- **output_schema:** For ``LLMTaskRunner`` only, a JSON schema that responses must conform to (by default returns ``{"result": {"type": "string"}}``). In ``DynamicLLMTaskRunner`` the ``output_schema`` is set dynamically at runtime. 
- **rules:** a set of rules that the task runner should use while executing. Rules are used to guide the behavior of the agent with system or developer prompts (optional).

### Class Definitions
Here's how to implement both ``LLMTaskRunner`` and ``DynamicLLMTaskRunner``. These classes handle the LLM execution loop, schema validation, and toolkit integration:

<CodeGroup>
    <LLMTaskRunners />
</CodeGroup>

### Service Implementation
Now that we've defined our classes, let's wrap each of them with a MeshAgent ``ServiceHost`` so we can call them into the room as agents. 
This example uses the  ``OpenAIResponsesAdapter`` with both the ``LLMTaskRunner`` (returns a fixed string response) and ``DynamicLLMTaskRunner`` (returns a schema defined at runtime).

<CodeGroup>
    <LLMTaskRunnerService />
</CodeGroup>

#### Running the Custom TaskRunners
From the terminal use the MeshAgent CLI to start the service and call both agents into the room: 

```bash bash 
meshagent setup #authenticate if not already connected
meshagent service run "main.py" --room=myroom
```

Next you can invoke the agents using the MeshAgent CLI, from code, or in the MeshAgent Studio.

**Option 1: Invoking from the CLI**

You can use the MeshAgent CLI directly to connect to the room and invoke either TaskRunner. 

<CodeGroup>
    ```bash LLMTaskRunner
    meshagent agents ask \
    --room myroom \
    --agent llmtaskrunner \
    --input '{"prompt":"Write a poem about ai agents"}'
    ```

    ```bash DynamicLLMTaskRunner
    REQUEST=$(cat <<'JSON'
    {
        "prompt": "Create a product listing for a bluetooth speaker",
        "output_schema": {
            "type": "object",
            "additionalProperties": false,
            "required": ["title", "price", "features", "description"],
            "properties": {
            "title": {"type": "string"},
            "price": {"type": "number"},
            "features": {"type": "array", "items": {"type": "string"}},
            "description": {"type": "string"}
            }
        }
    }
    JSON
    )

    meshagent agents ask \
    --room myroom \
    --agent dynamicllmtaskrunner \
    --input "$REQUEST"

    ```
</CodeGroup>

**Option 2: Invoking from Code**

You can use the MeshAgent Python SDK directly to connect to the room and invoke either TaskRunner. Paste the following code into a file called ``invoke_llm_taskrunner.py`` and edit it as applicable. By default this will invoke both the LLMTaskRunner and DynamicLLMTaskRunner using a default output schema defined in the file. 

Be sure you have already exported your ``MESHAGENT_API_KEY``. You can create and activate one by running ``meshagent api-key create <KEY_NAME> activate`` this will print the value of the key one time for you to copy. Then save it and export it ``export MESHAGENT_API_KEY=xxxxx``

<CodeGroup>
    <InvokeLLMTaskRunner />
</CodeGroup>

Now run the file: 
```bash bash
python invoke_llm_taskrunner.py
```

**Option 3 Invoking the LLMTaskRunner from the Studio**

1. Go to [studio.meshagent.com](https://studio.meshagent.com)
2. Enter the room, ``myroom``
3. Click menu —> **“Run Task”**
4. Select **LLM Task Runner** from the agent dropdown. (Optional) Click "add tools" and add tools to your new LLMTaskRunner
5. Enter a prompt
6. Results appear 

## Next Steps
Dive Deeper into TaskRunners
- [TaskRunner Overview](./taskrunner): Review what TaskRunners are and when to use them
- [Use an existing agent with a MeshAgent TaskRunner](./taskrunner_agents_and_tools): Learn how to take an existing PydanticAI Agent, use it as a TaskRunner, then use the TaskRunner as a tool for a MeshAgent ``ChatBot``
- [Prebuilt MeshAgent TaskRunners](./taskrunner_prebuilt): Learn how to use prebuilt TaskRunners that come out of the box with every MeshAgent room

Learn how to deploy agents with MeshAgent
- [Services & Containers](../../services_room_containers/overview): Understand different options for running, deploying, and managing agents with MeshAgent
- [Secrets & Registries](../../secrets/secrets_overview): Learn how to store credentials securely for deployment