---
title: "Worker"
---

import CliWorkerYaml from "/snippets/examples/cli/worker/meshagent.mdx"
import QueueWorker from "/snippets/examples/python/agents/queueworker/queueworker.mdx"
import PushQueue from "/snippets/examples/python/agents/queueworker/push_queue.mdx"


## Overview
A ``Worker`` is a specialized **queue-based agent** that processes messages sent to a MeshAgent room queue. **Other agents or
applications can push tasks to the queue and the Worker will handle them in the background**. This is helpful for
long‑running or asynchronous jobs that shouldn't block an interactive chat agent.

### Two ways to build a Worker
1. **CLI:** Run production-ready workers with a single command. Configure queue, tools, and rules using flags. Ideal for most use cases.
2. **SDK:** Extend the base ``Worker`` class with custom code when you need deeper integrations or specialized behaviors.

### In this guide you will learn
- When to use ``Worker``
- How to run and deploy a ``Worker`` with the MeshAgent CLI
- How to build and deploy a ``Worker`` with the MeshAgent SDK
- How the ``Worker`` works including constructor parameters, lifecycle, processing flow, and hooks

## When to use Worker
- Process background jobs pushed by apps or other agents
- Run long or repetitive jobs off the main chat thread
- Execute non-interactive tasks where no follow up questions are desired
- Batch operations that process multiple items sequentially
- Schedule or trigger work externally and have the agent pick it up

If you need an interactive assistant, use [ChatBot](./chatbot). For real-time speech, see [VoiceBot](./voicebot). For a callable function/agent pattern with structured schemas, see [TaskRunner](./taskrunner).

## Run and deploy a Worker with the CLI
### Step 1: Run a ``Worker`` from the CLI
Let's run a Worker that listens on a queue and can write files to room storage.

```bash bash
# Authenticate to MeshAgent if not already signed in
meshagent setup

# Call a worker into your room
meshagent worker join --room quickstart --agent-name worker --queue tasks --require-storage \
  --room-rules "agents/worker/rules.txt" --rule "Process queued tasks and save results to storage."
```

When you add the `--room-rules "agents/worker/rules.txt"` flag and supply a file path for the rules, the file will be created if it does not already exist, this file is relative to the room storage. The `--queue` flag is required.

> **Tip:** Use `meshagent worker join --help` to see all available tools and options.

### Step 2: Send work to the queue
```bash bash
meshagent room queue send --room=quickstart --queue=tasks \
  --json '{"prompt": "save a poem about ai to poem.txt"}'
```

### Step 3: Package and deploy the worker
Once your worker runs locally, package it as a service so it is always available in the room.

Create a `meshagent.yaml` file:
<CodeGroup>
    <CliWorkerYaml />
</CodeGroup>

Deploy it to your Room:
```bash bash
meshagent service create --file meshagent.yaml --room quickstart
```

The `--room` flag is optional. Without it, the worker is deployed at the project level and appears in all rooms in your project.

## Build and deploy a Worker with the SDK
### Example: Creating a Storage Queue Worker

The sample below shows a Worker that listens on a queue and writes files using the `StorageToolkit`. After starting the
service you can push a message to the queue to trigger the worker.

First create a python file, ``main.py``, and define our ``StorageWorker``:

<CodeGroup>
    <QueueWorker />
</CodeGroup>

#### Running the Worker

From your terminal inside an activated virtual environment start the service:
```bash bash
export WORKER_QUEUE=test
meshagent setup # authenticate to meshagent if not already
meshagent service run "main.py" --room=quickstart
```

#### Sending Work to the Queue

Now that our agent is running, let's send it some work!

**Option 1: Using the MeshAgent CLI**
Use the MeshAgent CLI to directly connect to the room, mint a token, and send a message to the queue.

```bash bash
meshagent room queue send --room=quickstart --queue=test \
  --json '{"instructions": "save a poem about ai to poem.txt"}'
```

**Option 2: Invoking the Queue from Code**
Create a python file ``push_queue.py`` and define our function to push a message to the queue.

<CodeGroup>
    <PushQueue />
</CodeGroup>

Make sure your service is still running (``meshagent service run "main.py" --room=quickstart``). From a different tab in your terminal you can run the python file we created to send a task to the queue.

```bash bash
python push_queue.py
```

You'll see logs for the queue activity in the terminal window where your service is running. To verify the results we'll head to MeshAgent Studio where we can see the .txt files.

#### Checking Results in the Studio
From [studio.meshagent.com](https://studio.meshagent.com) open the room ``quickstart`` and you'll see our poem about AI in the poems.txt file!


## How Worker Works
### Constructor Arguments
A ``Worker`` accepts everything from ``SingleRoomAgent`` (``name``, ``title``, ``description``, ``requires``, ``labels``) plus queue-specific and LLM based parameters.

| Argument       | Type                          | Description                                                                                            |
| -------------- | ----------------------------- | ------------------------------------------------------------------------------------------------------ |
| `queue`        | `str`                         | **Required.** Name of the room queue to listen for messages on.                                                       |
| `llm_adapter`  | `LLMAdapter`                  | The adapter to use so the agent works with an LLM. We recommend using the ``OpenAIResponsesAdapter`` from ``meshagent-openai``. |
| `tool_adapter` | `ToolResponseAdapter \| None` | Optional adapter to translate tool outputs into LLM/context messages. We recommend using the ``OpenAIResponsesToolResponseAdapter`` from ``meshagent-openai``.                                 |
| `toolkits`     | `list[Toolkit] \| None`       | Local toolkits always available to this worker (in addition to any `requires`).                        |
| `rules`        | `list[str] \| None`           | Optional system prompt/rules that guide the agent's behavior.                                      |
| `requires`     | `list[Requirement] \| None`   | Schemas/toolkits to install in the room before processing. You can use RequiredSchema, RequiredToolkit to use existing toolkits and schemas that have been registered with the room.                                            |

### Lifecycle Overview

- ``await start(room)``: Connects to the room, installs requirements, and starts the background run() loop that consumes messages from the configured queue.
- ``await stop()``: Signals the loop to stop, awaits the main task, then disconnects cleanly.
- ``room property``: Access the active ``RoomClient`` for queues, storage, messaging, and tools.

### Processing Flow

1. Wait for work (long-polling). The worker listens for jobs using: ``message = await room.queues.receive(name=self._queue, create=True, wait=True)``
    With ``wait=True``, the call long-polls the queue: instead of returning immediately when the queue is empty, it waits asynchronously for a short window until a message arrives (or the wait times out). This is more efficient than tight polling and ensures the worker can pick up jobs immediately when they’re published.

2. Build context and tools. Create a fresh chat context (``init_chat_context()``), apply any rules, and resolve toolkits from requires plus local toolkits.
3. Represent the job in context. By default, ``append_message_context(...)`` serializes the message as a user message (JSON). Override this to customize how the payload is injected.
4. Process the job. ``process_message(...)`` runs the task (default implementation calls your ``llm_adapter.next()`` with the prepared context and toolkits).
5. Handle errors & keep running. Errors are logged. On receive failures, the loop backs off exponentially and retries; otherwise it immediately waits for the next message.


### Key Behaviors and Hooks
- **Queue consumption:** ``run()`` long-polls the queue ``(receive(..., wait=True))``, creating it if needed.
- **Context building:** ``append_message_context(...)`` inserts job data into the chat context (default: dump JSON as a user message).
- **Job execution:** ``process_message(...)`` is the main hook: it prepares context, tools, and (by default) calls the LLM adapter’s ``.next()`` function to work through the task.
- **Resilience:** Errors are logged; ``run()`` applies an exponential backoff between receive retries.

### Key Methods

| Method                                                             | Description                                                        |
| ------------------------------------------------------------------ | ------------------------------------------------------------------ |
| `async def run(room)`                                              | Main loop: receives queue messages and processes them.             |
| `async def process_message(chat_context, room, message, toolkits)` | Core job handler—override for custom behavior.                     |
| `async def append_message_context(room, message, chat_context)`    | How a job is represented in the chat context (override to change). |
| `async def start(room)` / `async def stop()`                       | Start/stop the worker, manage background task.                     |


## Next Steps
Continue Learning about Agents and Explore a variety of other base agents including:
- [ChatBot](./chatbot) for conversational text-based agents
- [VoiceBot](./voicebot) for conversational speech/voice based agents
- [TaskRunner](./taskrunner) for agents that run in the background with defined inputs and outputs
- [MailWorker](./mailworker) for email based agents
